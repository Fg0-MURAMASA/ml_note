{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1630,  0.1536,  0.1381,  0.0982, -0.0385, -0.1175,  0.2744,  0.4624,\n",
       "         -0.2161, -0.0258],\n",
       "        [ 0.1083,  0.0801,  0.1259,  0.2404, -0.0359, -0.0806,  0.1981,  0.4020,\n",
       "         -0.2114,  0.0699]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.模型构造\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X)\n",
    "\n",
    "# nn.Sequential定义了一种特殊的Module\n",
    "# Module是所有层和神经网络的父类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Module 是 PyTorch 中构建神经网络的基础类。\n",
    "通过继承 nn.Module，可以定义模型的结构、前向传播逻辑，并自动管理参数。\n",
    "它是 PyTorch 模型开发的核心，支持模块化设计、设备管理、模型保存与加载等功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当执行 net(X) 时，PyTorch 会自动调用 nn.Module 的 __call__ 方法，而 __call__ 方法内部会调用你定义的 forward 方法。具体过程如下：\n",
    "\n",
    "net(X) 等价于 net.__call__(X)：\n",
    "nn.Module 类实现了 __call__ 方法，当实例化对象被调用时（如 net(X)），Python 会执行 __call__。\n",
    "__call__ 方法的作用：\n",
    "__call__ 方法在调用 forward 方法之前，会执行一些额外的逻辑（如钩子函数、前向传播的预处理等）。\n",
    "最终，__call__ 会调用 forward 方法，并将结果返回。\n",
    "forward 方法的执行：\n",
    "forward 方法是你定义的模型前向传播逻辑，它会根据输入 X 计算输出。\n",
    "因此，net(X) 实际上是通过 __call__ 间接调用了 forward(X)，并输出了结果。\n",
    "当你调用 net(X) 时，X 会被传递给 forward 方法。\n",
    "在 forward 方法中，X 通过隐藏层 self.hidden 和输出层 self.out 计算得到输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    #调用父类 nn.Module 的构造函数。确保 MySequential 类正确初始化，继承 nn.Module 的核心功能\n",
    "    def __init__(self, *args): # args: 用户传入的子模块列表\n",
    "        # MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10)) 中的三个模块会被打包为 args 元组。\n",
    "        super().__init__()\n",
    "        # 遍历传入的所有子模块（如 nn.Linear, nn.ReLU 等）\n",
    "        for block in args:\n",
    "            # 将每个子模块对象 block 作为键和值，添加到 self._modules 字典中\n",
    "            # self._modules 是 nn.Module 内置的字典，用于跟踪子模块\n",
    "            self._modules[block] = block\n",
    "\n",
    "    def forward(self, X):\n",
    "        for block in self._modules.values():\n",
    "            # block(X): 正向传播的基本操作, 输入：当前的输入张量 X;\n",
    "            # ​操作：将 X 传递给当前子模块 block，执行该模块的 forward 方法;\n",
    "            # ​输出：当前子模块处理后的结果会覆盖 X，作为下一个子模块的输入\n",
    "            X = block(X)\n",
    "        return X\n",
    "\n",
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在教我们通过继承nn.Module, 我们如何实现比单纯的Sequential更多的功能\n",
    "# 像这个函数就是在正向传播函数中执行了自定义的代码\n",
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        X = self.linear(X)\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()\n",
    "\n",
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 层,  nn.linear, Sequential都是nn.Module的子类, 我们可以嵌套着使用(下文是Sequential和linear的嵌套)\n",
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chimera (Sequential)\n",
    "├── NestMLP (自定义模块)\n",
    "│   ├── net (Sequential)\n",
    "│   │   ├── Linear(20, 64)\n",
    "│   │   ├── ReLU()\n",
    "│   │   ├── Linear(64, 32)\n",
    "│   │   └── ReLU()\n",
    "│   └── linear (Linear(32, 16))\n",
    "├── Linear(16, 20)\n",
    "└── FixedHiddenMLP (自定义模块)\n",
    "    ├── rand_weight (固定张量 20x20)\n",
    "    ├── linear (Linear(20, 20))\n",
    "    └── 包含固定权重操作和归一化逻辑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.参数管理\n",
    "# 关注一个多层感知机\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在PyTorch中，state_dict()是一个关键方法，用于获取神经网络模型中可学习参数的字典对象\n",
    "print(net[2].state_dict()) # 因为这个模型是linear, relu, linear, 所以net[2]就是取最后一层的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(net[2].bias))# ​作用：查看 bias 参数的类型。\n",
    "print(net[2].bias)# 打印 bias 参数的完整信息，包括值和梯度属性。\n",
    "print(net[2].bias.data)# 直接访问 bias 参数的数值张量​（不包含梯度信息）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net[2].weight.grad == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一次性访问所有参数\n",
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.state_dict()['2.bias'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们定义了一个由嵌套快组成的网络rgnet, 里面是一个block2和一个linear层,\n",
    "# 而block2里面又循环嵌套了4个block1分别命名为\"block0\"-\"block3\"\n",
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 4),\n",
    "                         nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 内置初始化\n",
    "def init_normal(m): # 正态分布初始化函数 init_normal\n",
    "    if type(m) == nn.Linear:# 仅对线性层的权重和偏置进行初始化\n",
    "        #作用：将权重 weight 初始化为均值为 0、标准差为 0.01 的正态分布\n",
    "        # ​下划线 _ 的含义：PyTorch 中以下划线结尾的函数表示原地修改（in-place）​，即直接修改张量值而不返回新对象\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_normal)# PyTorch 中 nn.Module.apply(fn) 会递归地对所有子模块​（包括嵌套模块）应用函数 fn\n",
    "net[0].weight.data[0], net[0].bias.data[0]\n",
    "\n",
    "def init_constant(m):# ​常量初始化函数 init_constant\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于Parameter\n",
    "​继承自 Tensor: Parameter 是 torch.Tensor 的子类，因此具备所有张量的操作功能（如数学运算、GPU 加速等）。\n",
    "​专为模型参数设计它是: PyTorch 中用于标识模型可学习参数的标记类，能够被优化器自动识别并更新。\n",
    "当将一个 Parameter 赋值给一个 nn.Module 的属性时，它会自动被注册到该模块的参数列表中，从而可以通过 module.parameters() 访问\n",
    "用途: 模型的可学习参数（权重、偏置）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对不同的块应用不同的初始化方法\n",
    "def xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "\n",
    "net[0].apply(xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义初始化\n",
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\n",
    "            \"Init\",\n",
    "            *[(name, param.shape) for name, param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight, -10, 10) # nn.init.uniform_(m.weight, -10, 10)：将权重初始化为 [-10, 10] 的均匀分布\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "        #m.weight.data.abs() >= 5 生成布尔掩码（True 表示绝对值 ≥5，False 表示 <5）；\n",
    "        #布尔掩码转换为 0/1 值后，与原权重相乘，也就是实现了将绝对值 <5 的权重置零的操作\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一些权重操作\n",
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 42\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共享参数\n",
    "shared = nn.Linear(8, 8) #先把需要共享参数的层构造出来\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), shared, nn.ReLU(), shared, #在定义层级时, 直接在需要共享的地方引入share就行\n",
    "                    nn.ReLU(), nn.Linear(8, 1))\n",
    "net(X)\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义层\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "# 自定义一个没有任何参数的层\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #  self.shift = nn.Parameter(torch.tensor(init_shift))  可学习参数,此函数中没有定义\n",
    "\n",
    "#forward 方法接收输入张量 X，计算其均值 X.mean()，并将每个元素减去该均值，使得输出数据的均值为零\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()\n",
    "\n",
    "layer = CenteredLayer()\n",
    "layer(torch.FloatTensor([1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自己定义的层也可以嵌套进其他层级结构中\n",
    "net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())\n",
    "\n",
    "Y = net(torch.rand(4, 8))\n",
    "Y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 带参数的层\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_units, units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units, units))\n",
    "        self.bias = nn.Parameter(torch.randn(units,))\n",
    "\n",
    "# forward功能: 线性变换 并 应用 ReLU 激活函数，将负值置零，保留正值，引入非线性\n",
    "    def forward(self, X):\n",
    "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
    "        return F.relu(linear)\n",
    "\n",
    "linear = MyLinear(5, 3)\n",
    "linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用\n",
    "linear(torch.rand(2, 5))\n",
    "net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))\n",
    "net(torch.rand(2, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\user\\default\\AppData\\Local\\Temp\\ipykernel_8176\\845183693.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  x2 = torch.load('x-file')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存文件\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "x = torch.arange(4)\n",
    "torch.save(x, 'x-file')\n",
    "\n",
    "x2 = torch.load('x-file')\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\user\\default\\AppData\\Local\\Temp\\ipykernel_8176\\1375692550.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  x2, y2 = torch.load('x-files')\n",
      "C:\\user\\default\\AppData\\Local\\Temp\\ipykernel_8176\\1375692550.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mydict2 = torch.load('mydict')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 存储一个张量列表，然后把它们读回内存\n",
    "y = torch.zeros(4)\n",
    "torch.save([x, y], 'x-files')\n",
    "x2, y2 = torch.load('x-files')\n",
    "(x2, y2)\n",
    "# 写入或读取从字符串映射到张量的字典\n",
    "mydict = {'x': x, 'y': y}\n",
    "torch.save(mydict, 'mydict')\n",
    "mydict2 = torch.load('mydict')\n",
    "mydict2"
   ]
  },
  {
   "attachments": {
    "QQ_1742615298553.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![QQ_1742615298553.png](attachment:QQ_1742615298553.png)可以看到我们存到并读到了这些数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.output = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn(size=(2, 20))\n",
    "Y = net(X)\n",
    "\n",
    "#pytorch中保存模型: 保存所有的参数\n",
    "#state_dict()可以获取神经网络模型中可学习参数的字典对象, 我们保存state_dict()就行\n",
    "torch.save(net.state_dict(), 'mlp.params')\n",
    "\n",
    "# 保存好了, 怎么load回来呢?\n",
    "# load回来的前提: 我们需要拿到模型的定义\n",
    "clone = MLP()\n",
    "# 载回保存的参数\n",
    "clone.load_state_dict(torch.load('mlp.params'))\n",
    "clone.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
