## 词向量的基本介绍

### 我们如何表示一个单词的意思？

定义: 含义(韦氏词典)
用一个词、词组等表示的意思。
一个人想要用文字、手势等来表达的思想。
作品、艺术等所表达的思想。
最常见的语言意义思维方式：
能指(符号)表示(思想或事物)=外延语义学

### 将单词表示为离散符号

在传统的自然语言处理中，我们将单词视为离散的符号：
酒店、会议、汽车旅馆--一个地点
意思是一个1，其余的0
这样的单词符号可以用一个热点向量来表示：
汽车旅馆=[000000000010000]
酒店=[000000010000000]
向量的维度=词汇表中的词数(这样的话, 我们需要一个很多维的向量; 而且, 这种向量并没有对词语之间联系和相似性的标识)

**我们需要在向量中加入实词相似性的考量**

### 用语境表征词语

分布语义学：通过经常出现在附近的单词给出一个词的意义
当一个单词w出现在文本中时，它的上下文是出现在附近(在固定大小的窗口内)的一组单词.
使用w的多个上下文来构建w的表示, 在这之中, 一个词是一种类型, 它表征着一个词在示例中的用法和含义

### 单词向量

我们为每个单词构建一个密集的向量，选择时使其向量与出现在相似的上下文中的单词的向量数值相似;

banking={0.286
				  0.792
				-0.177
				-0.107
				  0.109
				-0.542
				  0.349
			  	0.271}
注意：单词向量也称为单词嵌入或(神经)单词表示
它们是分布式表示法

所以说, 最终我们将大量的单词设置于一个高维的空间的各处

![img](file:///C:\user\default\AppData\Local\Temp\QQ_1746939122961.png)

### word2vec：概述

Word2vec是一个学习单词向量的框架
想法：
我们有一个很大的文本语料库(正文)
固定词汇表中的每个单词都由一个向量表示
浏览文本中的每个位置，其中有一个中心单词c和上下文(“外部”)单词o
使用c和o的单词向量的相似性来计算给定的中心词c计算上下文o出现的概率
不断调整单词向量以使得此概率接近给定语料库的概率

<img src="file:///C:\user\default\AppData\Local\Temp\QQ_1746939747448.png" alt="img" style="zoom:67%;" />

<img src="file:///C:\user\default\AppData\Local\Temp\QQ_1746939793265.png" alt="img" style="zoom: 67%;" />

**Word2vec**通过优化目标函数，学习词向量的表示，使得给定中心词时，模型能更准确地预测上下文词

对于每个位置t=1，..，T，在固定大小m的窗口内预测上下文单词，给定中心词wj。数据可能性:

该似然函数有时也称为“成本函数”或“损失函数”

![img](file:///C:\user\default\AppData\Local\Temp\QQ_1746940032899.png)

- θ代表所有要优化的变量
- 第一个累乘表示将每个位置的词当做中心词进行概率计算
- 第二个累乘表示对某个中心词而言, 对固定窗口内的临近词进行概率计算

**目标函数 J\(θ\)**：
采用平均负对数似然的形式

![img](file:///C:\user\default\AppData\Local\Temp\QQ_1746940627905.png)

- 便于计算的处理: 最小化 J(θ) 等价于最大化预测准确性。

目前的问题：如何计算P(wt+j|wt; θ)

- 我们将对每个单词w使用两个向量：
	当w是中心词时使用向量vw
	当w是上下文词时使用向量uw
- 那么，对于一个中心词c和其上下文词o, 可得:![img](file:///C:\user\default\AppData\Local\Temp\QQ_1746940932289.png)
	- *vc*：中心词 *c* 的词向量（输入向量）
	- *uo*：上下文词 *o* 的词向量（输出向量）
	- **分子**：exp(*u**o**T**v**c*) 表示中心词 *c* 和上下文词 *o* 的词向量内积的指数。内积越大，说明两者相关性越强
	- **分母**：对所有词汇表 *V* 中的词 *w* 计算 exp(*u**w**T**v**c*) 并求和，目的是归一化，使得概率总和为 1。
	- softmax(uoTuc)

训练:  通过调整模型参数 *θ*（所有参数的拼接向量），**最小化损失函数（Loss Function）**，从而提升模型预测准确性

![img](file:///C:\user\default\AppData\Local\Temp\QQ_1746941665453.png)

现在我们有一个巨大的参数θ, 它由每个词的两个向量u和v组成(作为中心词/作为上下文), 每个向量都是d维的, 总共有V个词, 所以θ是一个2dV维的向量.

当然, 我们使用梯度下降:

![img](file:///C:\user\default\AppData\Local\Temp\QQ_1746942271301.png)

以某一个vc为例:

对log P(o|c)的求导:

- log(exp(uoTvc)) - log(Σexp(uiTvc))求偏导
- uoTvc - (1/Σexp(uiTvc)) * Σ(exp(uiTvc)求偏导)
- ![img](file:///C:\user\default\AppData\Local\Temp\QQ_1746942910482.png)
- uo - Σsoftmax(uxTuc)*ux

词向量带来的一些功能

- 相似度判断: 点积就可以用于判断相似度, 十分简便

	![img](file:///C:\user\default\AppData\Local\Temp\QQ_1746943995517.png)

- 可以进行向量加减法:![img](file:///C:\user\default\AppData\Local\Temp\QQ_1746944050036.png)

	king - man + woman = Queen

	这就相当于 king - man = queen - woman 表示出了一个对等关系

	再举个例子就是: Australia - beer = France - champagne

## 依赖分析

**成分结构 = 短语结构语法 = 上下文无关文法（CFGs）**

语言中“嵌套成分”的组织方式，即较小的语言单位（如单词）通过规则组合成更大的成分（如短语），最终形成完整句子。

**起点**：单词（标注词性）

- 例：`the (Det)`, `cat (N)`, `cuddly (Adj)`, `by (P)`, `door (N)`
- 词性标注是分析句法结构的基础步骤。

**第一层组合**：单词→短语

- 例：`the cuddly cat`（Det + Adj + N → 名词短语NP）, `by the door`（P + Det + N → 介词短语PP）
- 体现短语结构规则（如NP → Det Adj N）。

**第二层组合**：短语→更复杂短语

- 例：`the cuddly cat by the door`（NP + PP → 更大的NP）
- 展示成分的递归性（短语可无限嵌套）。

### 依赖语法

依赖语法说明了那个单词依赖(修饰了, 关联了...)那个另一个单词; 很明显, 这种较为笼统的说法会使得一个句子有很多种不同的依赖选择, 所以说, 我们进一步地构筑了**普遍依赖框架**

> 为什么我们需要句子结构？
> 人类通过将单词组合成更大的单位来传达复杂的含义，从而交流复杂的思想
> 听众需要弄清楚 什么 修饰了[或者说"附加到"] 什么
> 模型需要理解句子结构才能正确地解释语言

根据不同的依赖, 一个句子可能有很多歧义:

![img](file:///C:\user\default\AppData\Local\Temp\QQ_1746968405432.png)

根据一些计算, 一段话中可能语言依赖的总数是指数级增长的

> Catalan数 *C**n* 表示对 *n* 个元素进行**合法括号化**（或**完全二叉树**）的组合数，其增长速度为 Ω(4*n*/*n*3/2)（指数级
>
> 若一种语言的句子通过**递归的上下文无关文法（CFG）**生成（如 `S → ( S ) S | ε`），其合法句子的总数随句子长度 *n* 按Catalan数增长。
>
> - 例：`"the cat saw the dog"` 可能有多种句法解析（歧义），解析树的数量即Catalan数。

一个小例子:

依存路径（Dependency Paths）
技术原理：
通过分析句子中词语间的依存关系（如 nsubj、nmod:with、conj:and 等），捕捉语义关联。例如：

- KaiC → interacts ← with → KaiA 的路径表明蛋白质 KaiC 与 KaiA 存在相互作用。
- 依存路径能绕过表面词序，直接提取深层语义关联

- The results demonstrated that KaiC rhythmically interacts with KaiA and KaiB."

	提取结果：

	- `KaiC ↔ KaiA`
	- `KaiC ↔ KaiB`

### 树库(treebank)

在训练时, 我们不仅要考虑词之间的关联性, 词句之间的依赖我们也要考虑; 所以说, 本着实现一个功能需要的不仅是范式, 还需要大量的训练数据的思想, 我们为训练构造一个树库, 这个库里是一个个句子产生的树, 树之中包含着每个词句的依赖关系;

### **依存句法分析（Dependency Parsing）**的信息来源

1. **双语词汇亲和力**-高亲和力搭配
	- **作用**：分析特定词对之间形成依存关系的概率
	- 示例：
		- 句子中 `discussion` 与 `of` 的高亲和力（`discussion of` 是常见搭配）
		- 图中标注 `[discussion]` 的合理性验证
2. **依存距离**-临近
	- **原则**：大多数依存关系发生在相邻或近距离词汇之间
	- 示例：
		- `Discussion` 与 `completed` 的距离较远（需跨越多词），因此依存优先级较低
		- 优先选择 `Discussion → of` 或 `issues → outstanding` 等局部依存
3. **插入成分**-符号
	- **限制**：依存关系通常不会被动词或标点符号隔断
	- 示例：
		- 句子中 `was completed` 是动词短语，可能阻断 `Discussion` 直接依存 `issues`
		- 图中标注说明插入成分（如动词、逗号）会影响依存决策
4. **中心词的价**- 考虑整体结构
	- **规则**：不同中心词（Head）对依存成分的数量和位置有固定偏好
	- 示例：
		- 动词 `completed` 通常需要主语（`Discussion`）和状语（`of...issues`），符合其"价"结构
		- 名词 `discussion` 常带介词短语修饰（如 `of...`）

### **依存句法分析的核心机制**

**唯一ROOT**：仅一个词直接依赖ROOT（图中 `ROOT → 做`）。

**无循环**：禁止双向依赖（如 `A→B` 和 `B→A` 同时存在）。

**树状结构**：所有依存关系构成有向无环图（DAG），最终形成树

英文：`I'll give a talk tomorrow on neural networks`

- `ROOT → do`（核心动词）

- `do ← will`（助动词）

- `will ← I`（主语）

- `give → talk`（宾语）
- `talk -> a`

- `talk ← on`（介词修饰）

- `on → networks`（介词宾语）

- `networks ← neural`（名词修饰）

### **句法投射性（Projectivity）**

当句子中的单词按线性顺序排列时，若所有依存弧（箭头）**不交叉**且位于单词上方，则该句法分析是**投射性的**

![img](file:///C:\user\default\AppData\Local\Temp\QQ_1747057050774.png)

图中例句 **"Who did Bill buy the coffee from yesterday?"** , 其黑色部分符号投射性, 但是这个句子包含**移位成分**（如疑问词 `Who` 前置），导致粉色弧线（`Who→from`）与黑色弧线（如 `buy→coffee`）**交叉**，形成非投射结构

非投射性依赖能准确捕捉疑问句、话题化等复杂结构的语义关系（如 `Who` 实际是 `from` 的宾语，需长距离依存）。

**贪婪基于转换的依存句法分析（Greedy Transition-Based Parsing）**

- **栈（σ）**：以 `ROOT` 符号开头，**右侧为栈顶**（图中示例为 `ROOT→word1→word2`）。
- **缓冲区（β）**：以输入句子开头，**左侧为栈顶**（如 `word3→word4→...`）。
- **依存弧集合（A）**：初始为空，动态存储生成的依存关系（如 `word2→word3`）。
- **动作集**：决定栈与缓冲区的操作（见下文）。

这张幻灯片的核心内容是**贪婪基于转换的依存句法分析（Greedy Transition-Based Parsing）**，以下是详细解析：

------

#### **一、算法核心机制**

1. **基本定义**（Nivre 2003）：
	- 一种**贪婪判别式依存解析器**，通过自底向上的动作序列构建依存树。
	- 类比移进-归约分析器，但“归约”动作专门用于生成**左中心**（head在左）或**右中心**（head在右）的依存关系。
2. **核心组件**：
	- **栈（σ）**：以 `ROOT` 符号开头，**右侧为栈顶**（图中示例为 `ROOT→word1→word2`）。
	- **缓冲区（β）**：以输入句子开头，**左侧为栈顶**（如 `word3→word4→...`）。
	- **依存弧集合（A）**：初始为空，动态存储生成的依存关系（如 `word2→word3`）。
	- **动作集**：决定栈与缓冲区的操作（见下文）。

------

#### **二、关键动作类型**

1. **移进（Shift）**：
	- 将缓冲区顶部的词移入栈顶。
	- **条件**：缓冲区非空。
	- **示例**：若栈顶为 `ROOT→A`，缓冲区为 `B→C`，执行 `Shift` 后栈变为 `ROOT→A→B`。
2. **左弧归约（Left-Arc）**：
	- 将栈顶词作为**依存子节点**，连接到栈中次顶词（head在右）。
	- **示例**：栈顶为 `A→B`，执行 `Left-Arc` 后生成依存弧 `B→A`，并弹出 `A`。
3. **右弧归约（Right-Arc）**：
	- 将栈中次顶词作为**依存子节点**，连接到栈顶词（head在左）。
	- **示例**：栈顶为 `A→B`，执行 `Right-Arc` 后生成依存弧 `A→B`，并弹出 `B`。

#### **三、算法流程示例**

**输入句子**：`ROOT A B C`

1. **初始状态**：
	- 栈 σ = `[ROOT]`
	- 缓冲区 β = `[A, B, C]`
	- 依存弧 A = `{}`
2. **步骤演示**：
	- **Shift**：σ = `[ROOT, A]`, β = `[B, C]`
	- **Shift**：σ = `[ROOT, A, B]`, β = `[C]`
	- **Left-Arc**：生成 `B→A`，σ = `[ROOT, B]`
	- **Right-Arc**：生成 `ROOT→B`，σ = `[ROOT]`
	- **Shift**：σ = `[ROOT, C]`, β = `[]`
	- **Right-Arc**：生成 `ROOT→C`，σ = `[ROOT]`
3. **最终输出**：
	- 依存弧 A = `{B→A, ROOT→B, ROOT→C}`

#### 如何选择下一步?---ML

**机器学习分类器+有限特征**实现了高效的依存解析动作选择

如何评估?

**Gold标准（正确答案）**

![img](file:///C:\user\default\AppData\Local\Temp\QQ_1747058690886.png)

- `obj` 误标为 `ccomp`（语义错误）
- `video` 本应为修饰词（`nn`），但被错误标注为 `nsubj`（非法关系）

**评估指标计算**

1. **未加权依存准确率（UAS）**：

	- **公式**：`UAS = #正确依存弧数 / #总依存弧数`

	- 

		计算

		：

		- 正确依存弧：`saw→She`, `saw→lecture`, `lecture→the`, `lecture→video`（仅结构正确，标签错误不计）
		- 总数：5（含ROOT→saw）
		- **UAS = 4/5 = 80%**（图中结果）

2. **标签准确率（LAS）**：

	- **隐含计算**：仅2/5依存弧完全正确（结构+标签），故 **LAS = 40%**（未直接显示）。

## 语言模型和RNN

### **神经网络的依存句法分析中特征提取方法**

- **栈(Stack)**：存储已处理的部分解析树（如 `[ROOT, has_VBZ, good_JJ]`）
- **缓冲区(Buffer)**：存放待处理的输入词（如 `[control_NN, He_PRP]`）
- **依存关系表**：记录已生成的依存标签（如 `nsubj`, `amod` 等）

**关键特征来源**（图中表格）：



| 位置标记 |  单词   | 词性(POS) | 依存关系(dep) |
| :------: | :-----: | :-------: | :-----------: |
|    S1    |  good   |    JJ     |     amod      |
|    S2    |   has   |    VBZ    |     root      |
|    b1    | control |    NN     |      obj      |
|  lc(s1)  |    -    |     -     |       -       |
|  rc(s1)  |   He    |    PRP    |     nsubj     |

二、神经表示构建

1. **向量化步骤**：
	- **单词嵌入**：每个词（如 `has`）映射为d维向量（如300维GloVe）
	- **POS嵌入**：词性标签（如 `VBZ`）单独编码为50维向量
	- **依存嵌入**：依存标签（如 `nsubj`）编码为20维向量

![img](file:///C:\user\default\AppData\Local\Temp\QQ_1747060550383.png)

### **基于图的依存句法分析器**

这张幻灯片详细介绍了**基于图的依存句法分析器（Graph-based Dependency Parsing）**的核心机制和实现原理，以下是专业解析：

1. **得分矩阵构建**

	- 对句子中的每个词*wi*，计算其作为其他词*wj*依存中心词（head）的得分*s*(*wj*→*wi*)

	- 

		示例得分矩阵

		（图中部分数据）：

		|  依存关系  | 得分 |
		| :--------: | :--: |
		| ROOT → sat | 0.5  |
		| cat → big  | 2.0  |
		| ROOT → big | 0.3  |

2. **全局优化目标**

	- 最大化整棵依存树的总得分：Score(*T*)=(*wj*→*wi*)∈*T*∑*s*(*wj*→*wi*)

3. **最小生成树（MST）求解**

	- 使用**Chu-Liu/Edmonds算法**找到最大生成树（将得分视为负权重）
	- 时间复杂度：*O*(*n*2)（n为句子长度）

关键技术细节

1. **上下文敏感的特征设计**
	- 传统方法使用手工特征模板（如距离、词性组合）
	- 现代神经网络方法：
		- 用BiLSTM编码上下文信息
		- 通过双线性变换计算得分：
		- ![img](file:///C:\user\default\AppData\Local\Temp\QQ_1747061268191.png)
2. **示例解析（图中"big"的选择）**
	- 候选中心词得分：
		- ROOT → big = 0.3
		- cat → big = 2.0
	- **正确选择**：由于2.0 > 0.3，确定"cat"作为"big"的中心词
	- 语言学解释：形容词("big")优先依附最近名词("cat")

### L2正则化的数学表达

正则化的详细信息, 可以看ipynb中的笔记

1. **完整损失函数**：

	*J*(*θ*)=交叉熵损失*N*1*i*=1∑*N*−log(∑*c*=1*C**e**f**c**e**f**y**i*)+L2正则项*λ**k*∑*θ**k*2

	参数说明：

	- *f**y**i*：样本*i*在真实类别上的得分
	- *λ*：正则化强度超参数（控制惩罚力度）
	- *θ**k*：模型第*k*个参数

2. **作用机理**：

	- 通过添加参数平方和惩罚项，强制模型保持较小的权重值
	- 抑制某些特征维度过度主导预测（如图中"many parameters"警告）

### Dropout的核心机制

dropout的详细信息, 可以看ipynb中的笔记

### 一些非线性使用的函数

![img](file:///C:\user\default\AppData\Local\Temp\QQ_1747108538706.png)

### 权重初始化

1. **权重初始化**
	- 小随机值必要性：
		- 若初始化为全零矩阵，所有神经元将学习相同特征（对称性破坏）
		- 数学表达：*W*∼Uniform(−*r*,*r*)，其中*r*为控制幅度的超参数
		- 典型范围：*r*=0.05（小网络）至*r*=0.01（深层网络）
2. **偏置初始化**
	- **隐藏层**：*b*=0（梯度更新自然打破对称）
	- 输出层：
		- 回归任务：初始化为目标均值*μ**y*
		- 分类任务：*b*=*σ*−1(*μ**y*)（如Sigmoid输出层取逆）

Xavier初始化（Glorot初始化）

1. **方差控制公式**：

	Var(*W**ij*)=*n*in+*n*out2

	- *n*in: 前一层神经元数（fan-in）
	- *n*out: 后一层神经元数（fan-out）

2. **物理意义**：

	- 保持信号在前向传播（FP）和反向传播（BP）中的方差稳定
	- 适用于tanh/Sigmoid等饱和激活函数

与其他技术的关联

1. **层归一化（LayerNorm）**：
	- 消除对初始化范围的严格依赖（通过归一化激活值分布）
	- 但初始化仍影响训练初期稳定性
2. **现代变体**：
	- **He初始化**：ReLU激活时方差改为2/*n*in（Kaiming初始化）
	- **正交初始化**：保持矩阵的正交性（适用于RNN）

讲了一下优化器, 学习率...

### 语言模型

预测各个词语在下文出现的概率:

![img](file:///C:\user\default\AppData\Local\Temp\QQ_1747109418961.png)

### 技术实现原理

​			**关键公式**（图片中粉色标注部分）

![img](file:///C:\user\default\AppData\Local\Temp\QQ_1747110008018.png)

- **链式法则**：将联合概率分解为**条件概率的连乘**
- **自回归特性**：每个词的概率依赖其之前的所有词（*x*(<*t*)）

1. **条件概率建模**

	- 传统方法（n-gram）：

		*P*(*x*(*t*)∣*x*(<*t*))≈*P*(*x*(*t*)∣*x*(*t*−1),…,*x*(*t*−*n*+1))

		- 仅考虑有限历史（n-1个词），通过语料库统计频率估算

	- 深度学习方法（RNN/Transformer）：

		*P*(*x*(*t*)∣*x*(<*t*))=softmax(*f**θ*(*x*(1),…,*x*(*t*−1)))

		- 神经网络 *f**θ* 编码全部历史信息
		- 输出层为词表上的概率分布（如GPT的token预测）

2. **训练目标**

	- 最大化对数似然：

	L=*t*=1∑*T*log*P*(*x*(*t*)∣*x*(<*t*))

	- 等价于最小化**交叉熵损失**（预测 vs 真实token）

### 语言模型构建

基于窗口的神经模型架构

![image-20250513123941105](C:\Users\范zs\AppData\Roaming\Typora\typora-user-images\image-20250513123941105.png)

**输入层**

- **形式**：4个连续单词的one-hot向量 x(1)x^{(1)}x(1)("the") 到 x(4)x^{(4)}x(4)("their")

- **维度**：![img](file:///C:\user\default\AppData\Local\Temp\QQ_1747111082982.png)

- **关键转换**：通过嵌入矩阵E获得词向量 

	![img](file:///C:\user\default\AppData\Local\Temp\QQ_1747111104951.png)

**嵌入拼接层**

![img](file:///C:\user\default\AppData\Local\Temp\QQ_1747111205636.png)

**隐藏层计算**

- 公式：

	h=f(We+b1)h = f(W e + b_1)h=f(We+b1)

	- W: 权重矩阵（图中蓝色箭头）
	- f: 激活函数（如ReLU，图中未显式标注）

- **物理意义**：提取输入序列的联合特征表示

**输出层**

- 公式：

	y=softmax(Uh+b2)y = \text{softmax}(U h + b_2)y=softmax(Uh+b2)

	- UUU: 输出权重矩阵（图中橙色箭头）
	- y∈R∣V∣y \in \mathbb{R}^{|V|}y∈R∣V∣: 词表上的概率分布（右侧柱状图）

- **实例标注**：柱状图显示"a"和"zoo"为高概率候选词

基于窗口的神经模型架构相比于n-gram, 有如下进步:

对n-gram LM的改进：

- 没有稀疏性问题
- 不需要存储所有观察到的n-gram

遗留问题：

- 固定窗口太小
- 扩大窗口就会扩大W
- x（1）和x（2）乘以W中完全不同的权重。输入处理方式不对称
- 需要一个可以处理任意长度输入的神经架构

于是RNN出现了:

关于RNN的更多信息, 可以看ipynb中的记录

![img](file:///C:\user\default\AppData\Local\Temp\QQ_1747111717520.png)

单词输入和上一个单词的隐藏层一起决定这一个单词的隐藏层

## Transformer

### 模型

- 自回归机制: 过去时刻的输出会作为当前时刻的输入(一个词一个词的生成)

- 基于encoder & decoder: 将自注意力, point-wise, 全连接层等结构堆在一起

![img](file:///C:\user\default\AppData\Local\Temp\QQ_1747282499077.png)



论文方法:

6个encoding层, 每个层中有两个子层: 1.多头自注意力+ (残差连接 & 层归一化) 2.全连接 + (残差连接 & 层归一化)

LayerNorm(x + Sublayer(x))

- batchNorm和layerNorm:
	- batchNorm:  逐特征分析, 沿feature行进
	- layerNorm: 逐样例分析, 沿batch行进

例子:

```python
假设输入为3个句子，每个句子填充到5个词，词向量维度为4：
shape = (batch_size=3, sequence_length=5, feature_dim=4)
[
  # 句子1（2个有效词 + 3个[PAD]）
  [[0.1, 0.3, -0.2, 0.5], [0.6, 0.0, 0.4, -0.1], [0.0, 0.0, 0.0, 0.0], ...],
  # 句子2（3个有效词 + 2个[PAD]）
  [[0.2, -0.1, 0.7, 0.3], [0.4, 0.5, -0.2, 0.1], [0.1, -0.3, 0.0, 0.4], ...],
  # 句子3（4个有效词 + 1个[PAD]）
  [[-0.3, 0.2, 0.5, 0.1], [0.7, -0.4, 0.2, 0.0], [0.0, 0.1, -0.1, 0.3], ...]
]
```

batchnorm就是一个特征一个特征的看, 那就是沿着词向量维度看;

layernorm就是一个样例一个样例的看, 那就是一个句子一个句子的看, 沿着batch看;

对于transformer, 每个句子的长度不一, 每次截取时, seq长度都会变化, 在计算均值和方差时, batchnorm切出来的长度变换较大的小批量中的均值方差抖动较大

![img](file:///C:\user\default\AppData\Local\Temp\QQ_1747284375776.png)

而LayerNorm对**每个样本的每个位置独立归一化**（沿特征维度计算），完全不受序列长度和batch大小的影响，更适合处理变长序列

6个decoding层, 每个层中有3个子层: 带掩码的自注意力; 从encoding和上一次自注意力读取输入的注意力; 全连接层

### 注意力

![image-20250515131518738](C:\Users\范zs\AppData\Roaming\Typora\typora-user-images\image-20250515131518738.png)

计算Query与所有Key的相似度（内积），得到注意力分数矩阵

除以Key维度（dₖ）的平方根，防止点积值过大导致Softmax梯度消失

Softmax归一化将分数转换为概率分布，权重和为1

![img](file:///C:\user\default\AppData\Local\Temp\QQ_1747286449623.png)

注意力中也使用了musk, 用极大的负数替换了还未出现的值(softmax出来就接近0), 防止后文影响当前预测

![img](file:///C:\user\default\AppData\Local\Temp\QQ_1747286739212.png)

但是, 单单使用点积注意力会导致"没有可以学的参数", 所以使用多头注意力, 给模型h次机会, 参照可学的权重w将输入投影到多个低维空间, 去匹配不同模式需要的函数, 最终整合起来

![img](file:///C:\user\default\AppData\Local\Temp\QQ_1747286723340.png)

#### transformer模型中的注意力

- 编码器中, 有一个注意力层, 我们将输入复制成三份, 作为key, query, value(自注意力)

	![img](file:///C:\user\default\AppData\Local\Temp\QQ_1747286863009.png)

这一步其实是寻找输入的语句内部, 互相间的关联度高的词语

- 解码器中有两个注意力层, 其中先出现的注意力层和编码器中几乎一样, 唯一不同时加入了mask

- 解码器中的另一个注意力层不再是自注意力, 其Key和Value来自于编码器的输出, Query来自于解码器下层注意力的输入, 这一部分就是在根据解码器的输入(成品的上文)从编码器(输入内容)中挑选出需要的(注意到的)内容;

### FFN

attention将整个句子中的序列信息进行了一次汇总, 对于attention后的每个词, 句子信息已经全部汇总到词向量中了, 所以此时我们对每一个词单独的使用全连接网络是可以的, 依然充分利用了信息

对序列中的**每个位置**（如句子中的每个词）**独立且相同地**应用相同的全连接网络，不依赖其他位置的信息（与自注意力互补）

通过两层线性变换和ReLU激活函数，增强模型的非线性表达能力，帮助捕捉更复杂的语义	

### Embedding

将词转换为词向量, 可以参考之前的word2vec

### Positional Embedding

attention不会主动的处理时序的信息, 如果打乱句子顺序, 结果会不变, 所以需要人为地添加词的位置, Positional Embedding在ipynb中有所记录

### 自注意力的好处

![img](file:///C:\user\default\AppData\Local\Temp\QQ_1747288122593.png)

- 一个计算需要等待的计算次数: O(1); 
- 一个信息从一个数据点到达另一个数据点的步数: O(1)[因为query是和所以的key进行运算的, 所以一对间隔很远的query和key也可以一次到达]
- 循环层在这两个方面表现并不好